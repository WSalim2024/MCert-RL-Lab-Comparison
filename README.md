<div align="center">

# ğŸ¤– RL Lab: Q-Learning vs. Policy Gradients

### **An Interactive Battleground for Reinforcement Learning**

*Visualize the Fundamental Trade-offs Between Value-Based and Policy-Based Methods*

---

![Python](https://img.shields.io/badge/Python-3.10-3776AB?style=for-the-badge&logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![RL](https://img.shields.io/badge/Reinforcement_Learning-Lab-purple?style=for-the-badge)


[![GitHub](https://img.shields.io/badge/GitHub-WSalim2024-181717?style=flat-square&logo=github)](https://github.com/WSalim2024)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/waqar-salim/)

<br>

[**Features**](#-key-features) Â· [**The Science**](#-the-science) Â· [**Installation**](#-installation) Â· [**Usage**](#-usage)

<br>

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   "The best way to understand RL is to watch two agents learn â€”              â•‘
â•‘    one by memorizing values, one by tuning a brain."                          â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

</div>

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [The Science](#-the-science)
- [Key Features](#-key-features)
- [The Grid World](#-the-grid-world)
- [Screenshots](#-screenshots)
- [Tech Stack](#-tech-stack)
- [Installation](#-installation)
- [Usage](#-usage)
- [Directory Structure](#-directory-structure)
- [Author](#-author)

---

## ğŸ¯ Project Overview

**RL Lab: Algorithm Arena** is an **educational laboratory** designed to visualize the fundamental trade-offs in Reinforcement Learning. It pits two philosophically distinct algorithms against each other in a controlled **5Ã—5 Grid World** environment.

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           THE ALGORITHM ARENA                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚                            ğŸŸï¸ GRID WORLD ARENA                                  â”‚
â”‚                                                                                 â”‚
â”‚                        â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚ ğŸ†â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚ â˜ ï¸â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚ â˜ ï¸â”‚   â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚ğŸ¤– â”‚   â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                                    â”‚
â”‚                                                                                 â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â”‚  ğŸ§  Q-LEARNING      â”‚  VS   â”‚  ğŸ§¬ POLICY GRADIENT â”‚                    â”‚
â”‚        â”‚                     â”‚       â”‚                     â”‚                    â”‚
â”‚        â”‚  "I memorize the    â”‚       â”‚  "I learn the       â”‚                    â”‚
â”‚        â”‚   value of every    â”‚       â”‚   probability of    â”‚                    â”‚
â”‚        â”‚   state-action"     â”‚       â”‚   every action"     â”‚                    â”‚
â”‚        â”‚                     â”‚       â”‚                     â”‚
â”‚        â”‚  ğŸ“Š Q-Table         â”‚       â”‚  ğŸ”® Neural Network  â”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                                 â”‚
â”‚                         WHO LEARNS FASTER? WHO WINS?                            â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Why This Project?

| Challenge | How RL Lab Solves It |
|-----------|---------------------|
| RL algorithms are abstract | **Visual dashboard** shows learning in real-time |
| Hard to compare methods | **Side-by-side race** with live reward graphs |
| Neural networks are "black boxes" | **Brain Scanner** reveals internal activations |
| Theory-practice gap | **Interactive sliders** let you experiment with hyperparameters |

---

## ğŸ”¬ The Science

### The Two Paradigms of Reinforcement Learning

RL algorithms can be broadly categorized into two families. This lab explores one representative from each:

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VALUE-BASED vs POLICY-BASED LEARNING                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   VALUE-BASED (Q-Learning)              POLICY-BASED (REINFORCE)                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚                                                                                 â”‚
â”‚   "How good is this                     "What should I                          â”‚
â”‚    state-action pair?"                   probably do here?"                     â”‚
â”‚                                                                                 â”‚
â”‚        State + Action                        State                              â”‚
â”‚             â”‚                                  â”‚                                â”‚
â”‚             â–¼                                  â–¼                                â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚      â”‚  Q-Table  â”‚                      â”‚  Neural   â”‚                           â”‚
â”‚      â”‚  (Lookup) â”‚                      â”‚  Network  â”‚                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚            â”‚                                  â”‚                                 â”‚
â”‚            â–¼                                  â–¼                                 â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚      â”‚  Q-Value  â”‚                      â”‚  Action   â”‚                           â”‚
â”‚      â”‚  (Number) â”‚                      â”‚  Probs    â”‚                           â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                                 â”‚
â”‚      Q(s,a) = 0.73                      Ï€(Up)=0.6, Ï€(Down)=0.1                  â”‚
â”‚                                         Ï€(Left)=0.1, Ï€(Right)=0.2              â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

---

### ğŸ§  Q-Learning (Value-Based)

**Philosophy:** Learn the *value* of every state-action pair, then act greedily.

<table>
<tr>
<td width="50%">

#### How It Works

1. Maintain a **Q-Table**: `Q[state][action]`
2. Take action, observe reward and next state
3. Update Q-value using **Bellman Equation**:

$$Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]$$

4. Choose action: Îµ-greedy (explore vs exploit)

</td>
<td width="50%">

#### Characteristics

| Property | Value |
|----------|-------|
| **Representation** | Tabular (Q-Table) |
| **Stability** | âœ… Very stable |
| **Sample Efficiency** | âœ… High |
| **Scalability** | âŒ Limited to discrete states |
| **Convergence** | âœ… Guaranteed (under conditions) |

</td>
</tr>
</table>

#### Q-Table Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Q-TABLE EXAMPLE                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   State    â”‚    â†‘ Up    â”‚   â†“ Down   â”‚   â† Left   â”‚   â†’ Right  â”‚               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚   (0,0)    â”‚    0.34    â”‚    0.12    â”‚    0.00    â”‚   [0.78]   â”‚ â† Best action â”‚
â”‚   (0,1)    â”‚    0.45    â”‚    0.23    â”‚    0.11    â”‚   [0.89]   â”‚               â”‚
â”‚   (1,2)    â”‚   [0.92]   â”‚    0.15    â”‚    0.33    â”‚    0.67    â”‚               â”‚
â”‚   ...      â”‚    ...     â”‚    ...     â”‚    ...     â”‚    ...     â”‚               â”‚
â”‚   (4,4)    â”‚    0.00    â”‚    0.00    â”‚    0.00    â”‚    0.00    â”‚ â† Goal state  â”‚
â”‚                                                                                 â”‚
â”‚   ğŸ“Š 25 states Ã— 4 actions = 100 Q-values to learn                              â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ§¬ Policy Gradients (REINFORCE)

**Philosophy:** Directly learn a *policy* (probability distribution over actions) using a neural network.

<table>
<tr>
<td width="50%">

#### How It Works

1. Neural network outputs **action probabilities**
2. Sample action from distribution: $a \sim \pi_\theta(s)$
3. Collect entire episode trajectory
4. Update network using **Policy Gradient Theorem**:

$$\nabla_\theta J(\theta) = \mathbb{E}\left[ \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t \right]$$

Where $G_t$ = cumulative future reward

</td>
<td width="50%">

#### Characteristics

| Property | Value |
|----------|-------|
| **Representation** | Neural Network |
| **Stability** | âš ï¸ Can be unstable |
| **Sample Efficiency** | âŒ Lower (needs more episodes) |
| **Scalability** | âœ… Handles continuous actions |
| **Convergence** | âš ï¸ May suffer catastrophic forgetting |

</td>
</tr>
</table>

#### Policy Network Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          POLICY NETWORK ARCHITECTURE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   INPUT                 HIDDEN LAYERS              OUTPUT                       â”‚
â”‚   â”€â”€â”€â”€â”€                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  x  â”‚â”€â”€â”€â”          â”‚             â”‚            â”‚   â†‘ Up      â”‚â”€â”€â–º 0.60      â”‚
â”‚   â”‚coordâ”‚   â”‚          â”‚   Dense     â”‚            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜   â”‚â”€â”€â”€â”€â”€â”€â–º   â”‚   (64)      â”‚â”€â”€â”€â”€â”€â”€â–º     â”‚   â†“ Down    â”‚â”€â”€â–º 0.10      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”   â”‚   ReLU   â”‚             â”‚   ReLU     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚   â”‚  y  â”‚â”€â”€â”€â”˜          â”‚   Dense     â”‚            â”‚   â† Left    â”‚â”€â”€â–º 0.10      â”‚
â”‚   â”‚coordâ”‚              â”‚   (32)      â”‚   Softmax  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜              â”‚             â”‚            â”‚   â†’ Right   â”‚â”€â”€â–º 0.20      â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                                 â”‚
â”‚   State: (2,3)         64 + 32 neurons            Action Probabilities          â”‚
â”‚   â†’ [2, 3]             with ReLU                  (sum to 1.0)                  â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### âš”ï¸ Head-to-Head Comparison

<div align="center">

| Aspect | Q-Learning | Policy Gradients |
|:-------|:----------:|:----------------:|
| **Learning Target** | State-Action Values | Action Probabilities |
| **Data Structure** | Q-Table (lookup) | Neural Network (function) |
| **Update Frequency** | Every step | End of episode |
| **Exploration** | Îµ-greedy | Stochastic sampling |
| **Stability** | âœ… Stable | âš ï¸ High variance |
| **Sample Efficiency** | âœ… Efficient | âŒ Needs more data |
| **Catastrophic Forgetting** | âŒ No | âœ… Possible |
| **Continuous Actions** | âŒ No | âœ… Yes |

</div>

---

## âœ¨ Key Features

<table>
<tr>
<td width="50%">

### ğŸ›ï¸ Interactive Dashboard

Built with **Streamlit** to adjust hyperparameters in real-time:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âš™ï¸ Hyperparameters         â”‚
â”‚                             â”‚
â”‚  Learning Rate (Î±)          â”‚
â”‚  [0.01]â”€â”€â”€â”€â—â”€â”€â”€â”€[0.5]       â”‚
â”‚           Î± = 0.1           â”‚
â”‚                             â”‚
â”‚  Discount Factor (Î³)        â”‚
â”‚  [0.5]â”€â”€â”€â”€â”€â—â”€â”€â”€â”€[0.99]      â”‚
â”‚           Î³ = 0.95          â”‚
â”‚                             â”‚
â”‚  Episodes                   â”‚
â”‚  [100]â”€â”€â”€â”€â”€â—â”€â”€â”€â”€[2000]      â”‚
â”‚         n = 500             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
<td width="50%">

### ğŸ“Š Live Race Visualization

Watch cumulative reward graphs update as both agents train **side-by-side**:

```
Cumulative Reward
    â”‚
 500â”œ         â”Œâ”€â”€â”€â”€â”€â”€ Q-Learning
    â”‚        /
 400â”œ       /    â”Œâ”€â”€â”€ Policy Gradient
    â”‚      /    /
 300â”œ     /    / (catching up)
    â”‚    /    /
 200â”œ   /    /
    â”‚  /    /
 100â”œ /    /
    â”‚/    /
   0â”œâ”€â”€â”€â”€/â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0   100   200   300   400
              Episodes
```

</td>
</tr>
<tr>
<td colspan="2">

### ğŸ§  The Brain Scanner â€” *Spy on the Neural Network*

A unique visualization that reveals the **Policy Network's internal confidence** in each action direction. Watch how the agent's "beliefs" evolve over training:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ğŸ§  THE BRAIN SCANNER                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   Neural Network Action Confidence at State (0,0)                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚                                                                                 â”‚
â”‚   EPISODE 10 (Random)          EPISODE 100              EPISODE 500 (Learned)   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                                                 â”‚
â”‚        â†‘ 23%                       â†‘ 18%                      â†‘ 5%              â”‚
â”‚         â”‚                           â”‚                          â”‚                â”‚
â”‚    â†â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’                 â†â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’                â†â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’           â”‚
â”‚   28%   â”‚  26%                12%   â”‚  45%               3%    â”‚  [87%]         â”‚
â”‚         â”‚                           â”‚                          â”‚                â”‚
â”‚        â†“ 23%                       â†“ 25%                      â†“ 5%              â”‚
â”‚                                                                                 â”‚
â”‚   "I have no idea"            "Right seems good"         "Go RIGHT! (87%)"      â”‚
â”‚   (uniform distribution)      (learning...)              (confident policy)     â”‚
â”‚                                                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                                 â”‚
â”‚   ğŸ’¡ INSIGHT: Watch the network's confidence shift from uniform to peaked       â”‚
â”‚              as it discovers the optimal path to the goal.                      â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</td>
</tr>
</table>

---

## ğŸ—ºï¸ The Grid World

The custom 5Ã—5 Grid World environment serves as the controlled arena:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           5Ã—5 GRID WORLD ENVIRONMENT                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚                        â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚ğŸ† â”‚  (4,4) = GOAL                      â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚+10â”‚  Reward: +10                       â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚â˜ ï¸ â”‚   â”‚   â”‚   â”‚  (1,3) = TRAP                      â”‚
â”‚                        â”‚   â”‚-5 â”‚   â”‚   â”‚   â”‚  Reward: -5                        â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚â˜ ï¸ â”‚   â”‚  (3,2) = TRAP                      â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚-5 â”‚   â”‚  Reward: -5                        â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚ğŸ¤– â”‚   â”‚   â”‚   â”‚   â”‚  (0,0) = START                     â”‚
â”‚                        â”‚ S â”‚-1 â”‚-1 â”‚-1 â”‚-1 â”‚  Step cost: -1                     â”‚
â”‚                        â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                                    â”‚
â”‚                                                                                 â”‚
â”‚   ACTIONS: â†‘ Up | â†“ Down | â† Left | â†’ Right                                     â”‚
â”‚                                                                                 â”‚
â”‚   REWARDS:                                                                      â”‚
â”‚   â€¢ Reach goal (ğŸ†): +10                                                        â”‚
â”‚   â€¢ Hit trap (â˜ ï¸): -5 (episode ends)                                            â”‚
â”‚   â€¢ Each step: -1 (encourages efficiency)                                       â”‚
â”‚   â€¢ Hit wall: Stay in place, -1                                                 â”‚
â”‚                                                                                 â”‚
â”‚   OPTIMAL PATH: (0,0) â†’ â†’ â†’ â†’ â†‘ â†‘ â†‘ â†‘ â†’ (4,4) = 9 steps, +1 total reward       â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¸ Screenshots

<div align="center">

### Dashboard Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         [SCREENSHOT PLACEHOLDER]                                â”‚
â”‚                                                                                 â”‚
â”‚                    ğŸ›ï¸ Interactive Dashboard with Live Training                  â”‚
â”‚                                                                                 â”‚
â”‚                         Add image: assets/dashboard.png                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Live Race Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         [SCREENSHOT PLACEHOLDER]                                â”‚
â”‚                                                                                 â”‚
â”‚                    ğŸ“Š Q-Learning vs Policy Gradient Reward Curves               â”‚
â”‚                                                                                 â”‚
â”‚                         Add image: assets/live_race.png                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Brain Scanner Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         [SCREENSHOT PLACEHOLDER]                                â”‚
â”‚                                                                                 â”‚
â”‚                    ğŸ§  Neural Network Action Confidence Evolution                â”‚
â”‚                                                                                 â”‚
â”‚                         Add image: assets/brain_scanner.png                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*Screenshots will be added after deployment.*

</div>

---

## ğŸ› ï¸ Tech Stack

<div align="center">

| Layer | Technology | Version | Purpose |
|:-----:|:----------:|:-------:|:--------|
| ğŸ | **Python** | 3.10 | Core runtime |
| ğŸ§  | **TensorFlow** | 2.x | Deep learning (Policy Network) |
| | | `Keras` | High-level neural network API |
| ğŸ–¥ï¸ | **Streamlit** | 1.28+ | Interactive dashboard |
| ğŸ”¢ | **NumPy** | 1.24+ | Q-Table operations |
| ğŸ“Š | **Matplotlib** | 3.7+ | Reward curve plotting |
| ğŸ“‹ | **Pandas** | 2.0+ | Data logging & export |

</div>

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          SYSTEM ARCHITECTURE                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         STREAMLIT DASHBOARD                             â”‚   â”‚
â”‚   â”‚                           (app.py)                                      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â”‚                             â”‚                              â”‚
â”‚                    â–¼                             â–¼                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚      Q-LEARNING           â”‚   â”‚    POLICY GRADIENT        â”‚                 â”‚
â”‚   â”‚    (q_learning.py)        â”‚   â”‚  (policy_gradient.py)     â”‚                 â”‚
â”‚   â”‚                           â”‚   â”‚                           â”‚                 â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                 â”‚
â”‚   â”‚  â”‚     Q-Table         â”‚  â”‚   â”‚  â”‚   Neural Network    â”‚  â”‚                 â”‚
â”‚   â”‚  â”‚   (NumPy array)     â”‚  â”‚   â”‚  â”‚   (TensorFlow)      â”‚  â”‚                 â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                 â”‚                               â”‚                               â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                 â”‚                                               â”‚
â”‚                                 â–¼                                               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                    â”‚       ENVIRONMENT         â”‚                                â”‚
â”‚                    â”‚    (environment.py)       â”‚                                â”‚
â”‚                    â”‚                           â”‚                                â”‚
â”‚                    â”‚    5Ã—5 Grid World         â”‚                                â”‚
â”‚                    â”‚    â€¢ state, action, rewardâ”‚                                â”‚
â”‚                    â”‚    â€¢ done flag            â”‚                                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¥ Installation

### Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| **Python** | 3.10+ | [Download](https://python.org) |
| **pip** | Latest | Included with Python |
| **Git** | Any | [Download](https://git-scm.com) |

### Setup Instructions

```bash
# Clone the repository
git clone https://github.com/WSalim2024/RL-Lab-Comparison.git

# Navigate to project directory
cd RL-Lab-Comparison

# Install dependencies
pip install -r requirements.txt

# Launch the Lab
streamlit run app.py
```

### requirements.txt

```
streamlit>=1.28.0
tensorflow>=2.12.0
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
```

---

## â–¶ï¸ Usage

### Launch the Dashboard

```bash
streamlit run app.py
```

### Access in Browser

```
Local URL: http://localhost:8501
```

### Recommended Experiments

| Experiment | Settings | What to Observe |
|------------|----------|-----------------|
| **Baseline** | Î±=0.1, Î³=0.95, 500 eps | Q-Learning converges faster |
| **High Learning Rate** | Î±=0.5 | Policy Gradient becomes unstable |
| **Low Discount** | Î³=0.5 | Both agents become short-sighted |
| **Long Training** | 2000 episodes | Policy Gradient eventually catches up |

---

## ğŸ“ Directory Structure

```
RL-Lab-Comparison/
â”‚
â”œâ”€â”€ ğŸ“„ app.py                    # Streamlit dashboard & comparison logic
â”œâ”€â”€ ğŸ“„ environment.py            # Custom 5Ã—5 Grid World engine
â”œâ”€â”€ ğŸ“„ q_learning.py             # Tabular Q-Learning implementation
â”œâ”€â”€ ğŸ“„ policy_gradient.py        # Deep Policy Gradient (REINFORCE)
â”œâ”€â”€ ğŸ“„ requirements.txt          # Python dependencies
â”œâ”€â”€ ğŸ“„ README.md                 # Project documentation
â””â”€â”€ ğŸ“„ .gitignore                # Git ignore rules
```

### Module Responsibilities

| File | Description |
|------|-------------|
| `app.py` | Main entry point; renders dashboard, orchestrates training |
| `environment.py` | Defines Grid World: states, actions, rewards, transitions |
| `q_learning.py` | Q-Table initialization, Îµ-greedy action selection, Bellman updates |
| `policy_gradient.py` | Keras model definition, episode collection, gradient computation |

---

## ğŸ”® Future Roadmap

| Feature | Description | Status |
|:--------|:------------|:------:|
| **DQN (Deep Q-Network)** | Neural network version of Q-Learning | ğŸ”œ Planned |
| **Actor-Critic** | Hybrid value + policy method | ğŸ”œ Planned |
| **Custom Grid Editor** | User-defined obstacles and goals | ğŸ”œ Planned |
| **Training Replay** | Step-by-step episode playback | ğŸ”œ Planned |

---

## ğŸ“š References

- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*
- Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning
- Mnih, V. et al. (2015). Human-level control through deep reinforcement learning

---

## ğŸ‘¨â€ğŸ’» Author

<div align="center">

### **Waqar Salim**

*Master's Student & IT Professional*

---

[![GitHub](https://img.shields.io/badge/GitHub-WSalim2024-181717?style=for-the-badge&logo=github)](https://github.com/WSalim2024)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/waqar-salim/)
---

**Built with ğŸ¤– algorithms, ğŸ§  neural networks, and ğŸ® curiosity**

*RL Lab: Algorithm Arena â€” Where Value Meets Policy*

---

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   "An agent is only as good as its representation of the world â€”             â•‘
â•‘    whether that's a table of values or a network of neurons."                 â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

</div>
