<div align="center">

# ğŸ§ª RL Lab: Algorithm Arena

### **A High-Fidelity Reinforcement Learning Workbench**

*Visualize the Fundamental Trade-offs Between Value-Based and Policy-Based Methods*

---

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

[![GitHub](https://img.shields.io/badge/GitHub-WSalim2024-181717?style=flat-square&logo=github)](https://github.com/WSalim2024)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/waqar-salim/)

<br>

[**Features**](#-key-features) Â· [**Architecture**](#-technical-architecture) Â· [**Installation**](#-installation-and-setup) Â· [**User Guide**](#-user-guide)

<br>

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   "The best way to understand RL is to watch two philosophies compete â€”      â•‘
â•‘    one learns by memorizing values, the other by tuning a neural brain."     â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

</div>

---

## ğŸ“‹ Table of Contents

1. [Overview](#-overview)
2. [Key Features](#-key-features)
3. [What This Project Is About](#-what-this-project-is-about)
4. [What It Does](#-what-it-does)
5. [What Is The Logic](#-what-is-the-logic)
6. [How Does It Work](#-how-does-it-work)
7. [What Are The Requirements](#-what-are-the-requirements)
8. [Technical Architecture](#-technical-architecture)
9. [Model Specifications](#-model-specifications)
10. [Tech Stack](#-tech-stack)
11. [Install Dependencies](#-install-dependencies)
12. [Installation and Setup](#-installation-and-setup)
13. [Launching the Cockpit](#-launching-the-cockpit)
14. [User Guide](#-user-guide)
15. [Restrictions and Limitations](#-restrictions-and-limitations)
16. [Disclaimer](#-disclaimer)
17. [Author](#-author)

---

## ğŸš€ Overview

**RL Lab: Algorithm Arena** is a high-fidelity Reinforcement Learning workbench designed to visualize the fundamental trade-offs between **Value-Based** (Q-Learning) and **Policy-Based** (Policy Gradients) methods.

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           THE ALGORITHM ARENA                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚                            ğŸŸï¸ GRID WORLD ARENA                                  â”‚
â”‚                                                                                 â”‚
â”‚                        â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚ ğŸ†â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚   â”‚ â˜ ï¸â”‚   â”‚   â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚   â”‚   â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                                    â”‚
â”‚                        â”‚ğŸ¤– â”‚   â”‚   â”‚   â”‚   â”‚                                    â”‚
â”‚                        â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                                    â”‚
â”‚                                                                                 â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â”‚  ğŸ§  Q-LEARNING      â”‚  VS   â”‚  ğŸ§¬ POLICY GRADIENT â”‚                    â”‚
â”‚        â”‚                     â”‚       â”‚                     â”‚                    â”‚
â”‚        â”‚  Tabular Method     â”‚       â”‚  Deep Learning      â”‚                    â”‚
â”‚        â”‚  25Ã—4 Q-Table       â”‚       â”‚  Neural Network     â”‚                    â”‚
â”‚        â”‚                     â”‚       â”‚                     â”‚                    â”‚
â”‚        â”‚  ğŸ“Š Lookup Table    â”‚       â”‚  ğŸ”® Function Approx â”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                                 â”‚
â”‚                         WHO LEARNS FASTER? WHO WINS?                            â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### The Two Paradigms

| Paradigm | Representative | Learning Target | Representation |
|:---------|:---------------|:----------------|:---------------|
| **Value-Based** | Q-Learning | State-Action Values | Tabular (Q-Table) |
| **Policy-Based** | REINFORCE | Action Probabilities | Neural Network |

---

## âœ¨ Key Features

<table>
<tr>
<td width="50%">

### ğŸ§  The Brain Scanner

A real-time visualization of the **Policy Network's internal activations**, showing how "confidence" in specific actions evolves over time.

```
Episode 10:          Episode 500:
   â†‘ 25%                â†‘ 3%
â†â”€â”€â”¼â”€â”€â†’ 25%         â†â”€â”€â”¼â”€â”€â†’ [91%]
   â†“ 25%                â†“ 3%

"Random guessing"    "Confident policy"
```

*Watch the neural network's decision-making sharpen from uniform randomness to peaked certainty.*

</td>
<td width="50%">

### ğŸ“Š Advanced Analytics

Tracks comprehensive metrics beyond simple rewards:

| Metric | Description |
|--------|-------------|
| **Efficiency** | Steps per Episode (lower = better) |
| **Success Rate** | Goal reached vs Pit fallen (%) |
| **Exploration Ratio** | Random vs Greedy actions |
| **Cumulative Reward** | Total reward over time |

</td>
</tr>
<tr>
<td width="50%">

### ğŸï¸ Live Algorithm Race

Side-by-side training visualization comparing:

- **Tabular Agent** (Q-Learning)
- **Deep Learning Agent** (Policy Gradient)

Watch convergence speed, stability, and performance unfold in real-time.

</td>
<td width="50%">

### âš™ï¸ Dynamic Tuning

Adjust hyperparameters on the fly via sidebar sliders:

- **Learning Rate** ($\alpha$): 0.001 - 0.5
- **Discount Factor** ($\gamma$): 0.5 - 0.99
- **Episodes**: 100 - 5000
- **Epsilon** (Q-Learning): 0.01 - 1.0

</td>
</tr>
</table>

---

## ğŸ“ What This Project Is About

This project **bridges the gap between theory and practice** by providing a visual "sandbox" to observe how different RL algorithms solve the same navigation problem differently.

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BRIDGING THEORY AND PRACTICE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   TEXTBOOK KNOWLEDGE                          VISUAL UNDERSTANDING              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚                                                                                 â”‚
â”‚   "Q-Learning uses the                        Watch the Q-Table                 â”‚
â”‚    Bellman Equation to                        values update in                  â”‚
â”‚    iteratively update                  â”€â”€â”€â–º   real-time as the                  â”‚
â”‚    state-action values"                       agent explores                    â”‚
â”‚                                                                                 â”‚
â”‚   "Policy Gradients can                       See the Brain Scanner             â”‚
â”‚    suffer from high                           show confidence                   â”‚
â”‚    variance and                        â”€â”€â”€â–º   oscillating during                â”‚
â”‚    instability"                               unstable training                 â”‚
â”‚                                                                                 â”‚
â”‚   ABSTRACT EQUATIONS                          CONCRETE VISUALIZATIONS           â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Learning Objectives

| Concept | How RL Lab Demonstrates It |
|---------|---------------------------|
| **Exploration vs Exploitation** | Îµ-greedy slider shows the trade-off |
| **Temporal Difference Learning** | Q-value updates visible step-by-step |
| **Policy Gradient Theorem** | Neural network confidence evolution |
| **Sample Efficiency** | Compare episodes needed to converge |
| **Stability vs Flexibility** | Q-Learning stability vs PG instability |

---

## âš¡ What It Does

The RL Lab performs three core functions:

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CORE FUNCTIONALITY                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚  1ï¸âƒ£ SIMULATE    â”‚    â”‚  2ï¸âƒ£ TRAIN       â”‚    â”‚  3ï¸âƒ£ VISUALIZE   â”‚            â”‚
â”‚   â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚            â”‚
â”‚   â”‚  5Ã—5 Grid World â”‚â”€â”€â”€â–ºâ”‚  Two Agents     â”‚â”€â”€â”€â–ºâ”‚  Live Graphs    â”‚            â”‚
â”‚   â”‚  Environment    â”‚    â”‚  Simultaneously â”‚    â”‚  & Analytics    â”‚            â”‚
â”‚   â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚            â”‚
â”‚   â”‚  â€¢ 25 States    â”‚    â”‚  â€¢ Q-Learning   â”‚    â”‚  â€¢ Rewards      â”‚            â”‚
â”‚   â”‚  â€¢ 4 Actions    â”‚    â”‚  â€¢ Policy Grad  â”‚    â”‚  â€¢ Efficiency   â”‚            â”‚
â”‚   â”‚  â€¢ Rewards      â”‚    â”‚  â€¢ Same Env     â”‚    â”‚  â€¢ Brain Scan   â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Specific Capabilities

1. **Simulates** a 5Ã—5 Grid World environment with configurable rewards
2. **Trains** two distinct agents simultaneously on identical conditions
3. **Renders** live performance graphs comparing:
   - Stability (reward variance)
   - Convergence speed (episodes to optimal)
   - Decision-making confidence (action probabilities)

---

## ğŸ§® What Is The Logic

### The World

A **5Ã—5 Grid** containing 25 discrete states:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           5Ã—5 GRID WORLD LOGIC                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   STATE NUMBERING:                      SPATIAL LAYOUT:                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚                                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                   â”‚
â”‚   â”‚ 20 â”‚ 21 â”‚ 22 â”‚ 23 â”‚ 24 â”‚           â”‚   â”‚   â”‚   â”‚   â”‚ğŸ† â”‚  State 24 = GOAL  â”‚
â”‚   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                   â”‚
â”‚   â”‚ 15 â”‚ 16 â”‚ 17 â”‚ 18 â”‚ 19 â”‚           â”‚   â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚
â”‚   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                   â”‚
â”‚   â”‚ 10 â”‚ 11 â”‚ 12 â”‚ 13 â”‚ 14 â”‚           â”‚   â”‚   â”‚â˜ ï¸ â”‚   â”‚   â”‚  State 12 = PIT   â”‚
â”‚   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                   â”‚
â”‚   â”‚  5 â”‚  6 â”‚  7 â”‚  8 â”‚  9 â”‚           â”‚   â”‚   â”‚   â”‚   â”‚   â”‚                   â”‚
â”‚   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                   â”‚
â”‚   â”‚  0 â”‚  1 â”‚  2 â”‚  3 â”‚  4 â”‚           â”‚ğŸ¤– â”‚   â”‚   â”‚   â”‚   â”‚  State 0 = START  â”‚
â”‚   â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                   â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Reward Structure

| Event | Reward | Effect |
|:------|:------:|:-------|
| ğŸ† **Reach Goal** (State 24) | **+10** | Episode ends (success) |
| â˜ ï¸ **Fall in Pit** (State 12) | **-10** | Episode ends (failure) |
| ğŸš¶ **Each Step** | **-1** | Encourages efficiency |
| ğŸ§± **Hit Wall** | **-1** | Stay in place |

### The Objective

**Maximize cumulative reward** by finding the shortest path to the goal while avoiding the center pit.

$$\text{Objective: } \max \sum_{t=0}^{T} \gamma^t r_t$$

---

## âš™ï¸ How Does It Work

### Q-Learning (Value-Based)

Uses a **lookup table** (25Ã—4) and the **Bellman Equation** to memorize the value of every state-action pair.

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Q-LEARNING MECHANISM                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   Q-TABLE (25 states Ã— 4 actions):                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                                                 â”‚
â”‚   State â”‚   â†‘ Up   â”‚  â†“ Down  â”‚  â† Left  â”‚ â†’ Right  â”‚                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                          â”‚
â”‚     0   â”‚   0.34   â”‚   0.12   â”‚   0.00   â”‚  [0.78]  â”‚ â† Best action            â”‚
â”‚     1   â”‚   0.45   â”‚   0.23   â”‚   0.11   â”‚  [0.89]  â”‚                          â”‚
â”‚    ...  â”‚   ...    â”‚   ...    â”‚   ...    â”‚   ...    â”‚                          â”‚
â”‚    24   â”‚   0.00   â”‚   0.00   â”‚   0.00   â”‚   0.00   â”‚ â† Terminal (Goal)        â”‚
â”‚                                                                                 â”‚
â”‚   UPDATE RULE (Bellman Equation):                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚                                                                                 â”‚
â”‚   Q(s,a) â† Q(s,a) + Î± [ r + Î³ max Q(s',a') - Q(s,a) ]                          â”‚
â”‚                         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚                              TD Target                                          â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Policy Gradient (REINFORCE)

Uses a **Neural Network** to output a probability distribution over actions, optimized via the **REINFORCE algorithm** (Monte Carlo Policy Gradient).

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        POLICY GRADIENT MECHANISM                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   NEURAL NETWORK FORWARD PASS:                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                                 â”‚
â”‚   State 7        One-Hot           Hidden         Output                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€                        â”‚
â”‚                                                                                 â”‚
â”‚     7      â–º   [0,0,0,0,0,    â–º   Dense(24)  â–º   Ï€(â†‘) = 0.15                   â”‚
â”‚                 0,0,1,0,0,        ReLU           Ï€(â†“) = 0.10                   â”‚
â”‚                 0,0,0,0,0,                       Ï€(â†) = 0.05                   â”‚
â”‚                 0,0,0,0,0,        Softmax        Ï€(â†’) = 0.70                   â”‚
â”‚                 0,0,0,0,0]                                                      â”‚
â”‚                                                                                 â”‚
â”‚   Size: 25      Input: 25         24 neurons     Output: 4                      â”‚
â”‚                                                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                                 â”‚
â”‚   UPDATE RULE (Policy Gradient Theorem):                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚                                                                                 â”‚
â”‚   âˆ‡Î¸ J(Î¸) = E[ Î£t âˆ‡Î¸ log Ï€(at|st) Â· Gt ]                                       â”‚
â”‚                                                                                 â”‚
â”‚   Where Gt = Î£k Î³^k r(t+k) (Return from time t)                                â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Head-to-Head Comparison

| Aspect | Q-Learning | Policy Gradient |
|:-------|:----------:|:---------------:|
| **Representation** | 25Ã—4 Table (100 values) | Neural Network (~700 params) |
| **Update Timing** | Every step (TD) | End of episode (MC) |
| **Exploration** | Îµ-greedy | Stochastic sampling |
| **Stability** | âœ… Very stable | âš ï¸ High variance |
| **Sample Efficiency** | âœ… High | âŒ Lower |
| **Scalability** | âŒ Limited | âœ… Handles large spaces |

---

## ğŸ“¦ What Are The Requirements

### System Requirements

| Requirement | Specification |
|:------------|:--------------|
| **Python** | 3.10 or higher |
| **OS** | Windows, macOS, or Linux |
| **RAM** | 4GB minimum (8GB recommended) |
| **Internet** | Required for initial package installation |

### Software Dependencies

All dependencies are installable via pip (see [Install Dependencies](#-install-dependencies)).

---

## ğŸ—ï¸ Technical Architecture

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          SYSTEM ARCHITECTURE                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         STREAMLIT FRONTEND                              â”‚   â”‚
â”‚   â”‚                           (app.py)                                      â”‚   â”‚
â”‚   â”‚                                                                         â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚  Sidebar    â”‚  â”‚  Reward     â”‚  â”‚   Brain     â”‚  â”‚  Advanced   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚  Controls   â”‚  â”‚  Graphs     â”‚  â”‚  Scanner    â”‚  â”‚  Metrics    â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                             â”‚
â”‚                                   â”‚ Orchestrates                                â”‚
â”‚                                   â–¼                                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                    â”‚                             â”‚                              â”‚
â”‚                    â–¼                             â–¼                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚      Q-LEARNING           â”‚   â”‚    POLICY GRADIENT        â”‚                 â”‚
â”‚   â”‚    (q_learning.py)        â”‚   â”‚  (policy_gradient.py)     â”‚                 â”‚
â”‚   â”‚                           â”‚   â”‚                           â”‚                 â”‚
â”‚   â”‚  â€¢ Q-Table (NumPy)        â”‚   â”‚  â€¢ Keras Sequential       â”‚                 â”‚
â”‚   â”‚  â€¢ Îµ-greedy selection     â”‚   â”‚  â€¢ REINFORCE algorithm    â”‚                 â”‚
â”‚   â”‚  â€¢ Bellman updates        â”‚   â”‚  â€¢ Gradient ascent        â”‚                 â”‚
â”‚   â”‚                           â”‚   â”‚                           â”‚                 â”‚
â”‚   â”‚  Returns: {               â”‚   â”‚  Returns: {               â”‚                 â”‚
â”‚   â”‚    'rewards': [...],      â”‚   â”‚    'rewards': [...],      â”‚                 â”‚
â”‚   â”‚    'lengths': [...],      â”‚   â”‚    'lengths': [...],      â”‚                 â”‚
â”‚   â”‚    'success_rate': [...], â”‚   â”‚    'success_rate': [...], â”‚                 â”‚
â”‚   â”‚    'expl_ratio': [...]    â”‚   â”‚    'expl_ratio': [...]    â”‚                 â”‚
â”‚   â”‚  }                        â”‚   â”‚  }                        â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                 â”‚                               â”‚                               â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                 â”‚                                               â”‚
â”‚                                 â–¼                                               â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚                    â”‚     GRID ENVIRONMENT      â”‚                                â”‚
â”‚                    â”‚    (environment.py)       â”‚                                â”‚
â”‚                    â”‚                           â”‚                                â”‚
â”‚                    â”‚  â€¢ 5Ã—5 Grid (25 states)   â”‚                                â”‚
â”‚                    â”‚  â€¢ 4 Actions (â†‘â†“â†â†’)       â”‚                                â”‚
â”‚                    â”‚  â€¢ Reward logic           â”‚                                â”‚
â”‚                    â”‚  â€¢ Episode management     â”‚                                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Component Responsibilities

| Component | File | Responsibility |
|:----------|:-----|:---------------|
| **Frontend** | `app.py` | UI rendering, training orchestration, visualization |
| **Engine** | `environment.py` | Custom GridEnvironment with state transitions |
| **Q-Agent** | `q_learning.py` | Tabular learning, returns metrics dictionary |
| **PG-Agent** | `policy_gradient.py` | Neural network training, returns metrics dictionary |

---

## ğŸ¤– Model Specifications

### Q-Learning Agent

| Property | Specification |
|:---------|:--------------|
| **Type** | Tabular (Non-parametric) |
| **Structure** | 25 states Ã— 4 actions = **100 Q-values** |
| **Update Rule** | Temporal Difference (TD-0) |
| **Action Selection** | Îµ-greedy |
| **Convergence** | Guaranteed (under conditions) |

### Policy Gradient Agent

| Property | Specification |
|:---------|:--------------|
| **Type** | Deep Neural Network (Parametric) |
| **Framework** | TensorFlow 2.x / Keras |
| **Architecture** | Sequential model |

**Network Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       POLICY NETWORK ARCHITECTURE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   LAYER              SPECIFICATION              OUTPUT SHAPE                    â”‚
â”‚   â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚
â”‚                                                                                 â”‚
â”‚   Input              One-hot encoded state      (None, 25)                      â”‚
â”‚                      Size: 25                                                   â”‚
â”‚                                                                                 â”‚
â”‚   Hidden             Dense(24, activation='relu')                               â”‚
â”‚                      24 neurons with ReLU       (None, 24)                      â”‚
â”‚                                                                                 â”‚
â”‚   Output             Dense(4, activation='softmax')                             â”‚
â”‚                      4 action probabilities     (None, 4)                       â”‚
â”‚                                                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                                 â”‚
â”‚   Total Parameters: (25 Ã— 24) + 24 + (24 Ã— 4) + 4 = 724 trainable params       â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

<div align="center">

| Layer | Technology | Version | Purpose |
|:-----:|:----------:|:-------:|:--------|
| ğŸ | **Python** | 3.10+ | Core runtime |
| ğŸ§  | **TensorFlow** | 2.x | Deep learning (Policy Network) |
| ğŸ”¢ | **NumPy** | Latest | Q-Table operations, array math |
| ğŸ“Š | **Matplotlib** | Latest | Reward curves, visualizations |
| ğŸ“‹ | **Pandas** | Latest | Data logging, metrics tracking |
| ğŸ–¥ï¸ | **Streamlit** | Latest | Interactive dashboard UI |

</div>

---

## ğŸ“¥ Install Dependencies

Create a `requirements.txt` file with the following contents:

```
numpy
matplotlib
tensorflow
pandas
streamlit
```

Or install directly:

```bash
pip install numpy matplotlib tensorflow pandas streamlit
```

---

## ğŸ”§ Installation and Setup

### Step 1: Clone the Repository

```bash
git clone https://github.com/WSalim2024/MCert-RL-Lab-Comparison.git
```

### Step 2: Navigate to Project Directory

```bash
cd MCert-RL-Lab-Comparison
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Verify Installation

```bash
python -c "
import numpy
import tensorflow
import streamlit
import matplotlib
import pandas

print('âœ… All dependencies installed successfully!')
print(f'   TensorFlow: {tensorflow.__version__}')
print(f'   NumPy: {numpy.__version__}')
"
```

---

## â–¶ï¸ Launching the Cockpit

### Start the Dashboard

```bash
streamlit run app.py
```

### Access in Browser

```
Local URL: http://localhost:8501
Network URL: http://192.168.x.x:8501
```

---

## ğŸ“– User Guide

### Step-by-Step Instructions

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER WORKFLOW                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                 â”‚
â”‚   STEP 1                    STEP 2                    STEP 3                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€                    â”‚
â”‚                                                                                 â”‚
â”‚   âš™ï¸ Configure              ğŸ§  Set Brain Scanner      ğŸ Start Race             â”‚
â”‚                                                                                 â”‚
â”‚   Use Sidebar to set:       Select a state to        Click "Start              â”‚
â”‚   â€¢ Episodes (e.g., 500)    "spy on"                 Training Race"            â”‚
â”‚   â€¢ Learning Rate (Î±)                                                          â”‚
â”‚   â€¢ Discount Factor (Î³)     Recommended:             Watch both agents         â”‚
â”‚                             Start State 0            train side-by-side        â”‚
â”‚                                                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                                 â”‚
â”‚   STEP 4                                                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€                                                                        â”‚
â”‚                                                                                 â”‚
â”‚   ğŸ“Š Analyze Results                                                            â”‚
â”‚                                                                                 â”‚
â”‚   Open "Advanced Metrics" dropdown to view:                                     â”‚
â”‚   â€¢ Efficiency (steps per episode)                                              â”‚
â”‚   â€¢ Success Rate (goal reached %)                                               â”‚
â”‚   â€¢ Exploration Ratio                                                           â”‚
â”‚                                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Recommended Experiments

| Experiment | Settings | Observation |
|:-----------|:---------|:------------|
| **Baseline** | Î±=0.1, Î³=0.95, 500 eps | Q-Learning converges faster |
| **High LR** | Î±=0.5 | Policy Gradient may diverge |
| **Long Training** | 2000 episodes | PG eventually catches up |
| **Low Discount** | Î³=0.5 | Both become short-sighted |

---

## ğŸ“¸ Screenshots

<div align="center">

### Dashboard Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         [SCREENSHOT PLACEHOLDER]                                â”‚
â”‚                                                                                 â”‚
â”‚                    ğŸ–¥ï¸ Main Dashboard with Sidebar Controls                      â”‚
â”‚                       Live Training Visualization                               â”‚
â”‚                                                                                 â”‚
â”‚                         Add image: assets/dashboard.png                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Brain Scanner Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         [SCREENSHOT PLACEHOLDER]                                â”‚
â”‚                                                                                 â”‚
â”‚                    ğŸ§  Neural Network Action Confidence Evolution                â”‚
â”‚                       Watch Policy Sharpen Over Training                        â”‚
â”‚                                                                                 â”‚
â”‚                         Add image: assets/brain_scanner.png                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Advanced Metrics Panel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         [SCREENSHOT PLACEHOLDER]                                â”‚
â”‚                                                                                 â”‚
â”‚                    ğŸ“Š Efficiency, Success Rate & Exploration Analysis           â”‚
â”‚                       Detailed Performance Breakdown                            â”‚
â”‚                                                                                 â”‚
â”‚                         Add image: assets/advanced_metrics.png                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

*To add screenshots: Create an `assets/` folder and save your Streamlit app screenshots there.*

</div>

---

## âš ï¸ Restrictions and Limitations

| Limitation | Description | Reason |
|:-----------|:------------|:-------|
| **Grid Size** | Fixed to 5Ã—5 | Optimized for visualization clarity |
| **Compute** | CPU-optimized | High episode counts (>5000) may slow browser rendering |
| **PG Stability** | May occasionally diverge | Demonstrates real RL instability (feature, not bug!) |
| **No GPU** | TensorFlow runs on CPU | Small network doesn't benefit from GPU |

### Catastrophic Forgetting Warning

> âš ï¸ **The Policy Gradient agent may occasionally diverge** (crash in performance) if the Learning Rate is set too high. This is **intentional** â€” it demonstrates a fundamental challenge in deep RL: instability and catastrophic forgetting.

---

## ğŸ“œ Disclaimer

<div align="center">

---

**ğŸ“ EDUCATIONAL USE ONLY**

---

</div>

This tool is designed for **educational purposes**. Reinforcement Learning is inherently **stochastic** â€” results may vary slightly between runs due to random seed initialization.

- **Not for Production**: This is a learning tool, not a production RL system
- **Variability Expected**: Different runs may produce different learning curves
- **Simplified Environment**: The 5Ã—5 Grid World is intentionally simple for pedagogical clarity

---

## ğŸ‘¨â€ğŸ’» Author

<div align="center">

### **Waqar Salim**

*Master's Student & IT Professional*

---

[![GitHub](https://img.shields.io/badge/GitHub-WSalim2024-181717?style=for-the-badge&logo=github)](https://github.com/WSalim2024)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=for-the-badge&logo=linkedin)](https://www.linkedin.com/in/waqar-salim/)

---

**Built with ğŸ¤– algorithms, ğŸ§  neural networks, and ğŸ® curiosity**

*RL Lab: Algorithm Arena â€” Where Value Meets Policy*

---

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                               â•‘
â•‘   "Reinforcement Learning is the science of making decisions under           â•‘
â•‘    uncertainty â€” and this lab lets you watch that uncertainty unfold."        â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

</div>
